import { NextRequest, NextResponse } from 'next/server';
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

// Extract text from .docx using mammoth
async function extractTextFromDocx(buffer: Buffer): Promise<string> {
  const mammoth = await import('mammoth');
  const result = await mammoth.extractRawText({ buffer });
  return result.value;
}

// Split text into meaningful paragraphs
function splitIntoParagraphs(text: string): string[] {
  return text
    .split(/\n{2,}/)
    .map(p => p.replace(/\n/g, ' ').trim())
    .filter(p => p.length > 60); // skip very short fragments
}

export async function POST(req: NextRequest) {
  try {
    const formData = await req.formData();
    const file = formData.get('file') as File | null;

    if (!file) {
      return NextResponse.json({ error: 'No file provided' }, { status: 400 });
    }

    const allowedTypes = [
      'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
      'application/msword',
    ];
    const allowedExtensions = ['.docx', '.doc'];
    const fileName = file.name.toLowerCase();
    const hasValidExt = allowedExtensions.some(ext => fileName.endsWith(ext));

    if (!hasValidExt && !allowedTypes.includes(file.type)) {
      return NextResponse.json(
        { error: 'Only .docx and .doc files are supported' },
        { status: 400 }
      );
    }

    // Read file buffer
    const arrayBuffer = await file.arrayBuffer();
    const buffer = Buffer.from(arrayBuffer);

    // Extract text
    let rawText: string;
    try {
      rawText = await extractTextFromDocx(buffer);
    } catch {
      return NextResponse.json(
        { error: 'Failed to parse document. Make sure it is a valid .docx file.' },
        { status: 422 }
      );
    }

    if (!rawText || rawText.trim().length < 100) {
      return NextResponse.json(
        { error: 'Document appears to be empty or too short to analyze (minimum 100 characters).' },
        { status: 422 }
      );
    }

    // Truncate if too long (Claude context limit)
    const maxChars = 40000;
    const truncated = rawText.length > maxChars;
    const textToAnalyze = truncated ? rawText.slice(0, maxChars) : rawText;

    const paragraphs = splitIntoParagraphs(textToAnalyze);

    if (paragraphs.length === 0) {
      return NextResponse.json(
        { error: 'Could not extract meaningful paragraphs from the document.' },
        { status: 422 }
      );
    }

    // Build the analysis prompt
    const prompt = `You are an expert forensic linguist specializing in detecting AI-generated text. Your task is to analyze the following document and determine what percentage was likely generated by an AI language model (such as ChatGPT, Claude, Gemini, etc.) versus written by a human.

DOCUMENT TEXT:
---
${textToAnalyze}
---

PARAGRAPHS TO ANALYZE INDIVIDUALLY:
${paragraphs.map((p, i) => `[${i}] ${p}`).join('\n\n')}

Analyze the text for these AI-generation signals:
- Unnaturally uniform sentence length and structure
- Overuse of transitional phrases ("Furthermore", "Moreover", "In conclusion", "It is worth noting")
- Hedge language patterns ("It is important to", "One might argue", "It should be noted")
- Lack of personal anecdotes, specific lived experiences, or genuine emotion
- Suspiciously perfect grammar and punctuation throughout
- Generic, non-specific claims without personal perspective
- Repetitive semantic patterns across paragraphs
- Overly balanced "on one hand / on the other hand" structures
- Absence of colloquialisms, typos, or natural human writing quirks
- Topic transitions that are too smooth and structured

Respond ONLY with a valid JSON object in this exact format (no markdown, no explanation outside JSON):

{
  "overall_score": <integer 0-100>,
  "verdict": "<one of: 'Likely Human', 'Probably Human', 'Mixed', 'Probably AI', 'Likely AI'>",
  "confidence": "<one of: 'Low', 'Medium', 'High'>",
  "word_count": <integer>,
  "summary": "<2-3 sentence summary of your findings>",
  "key_indicators": [<list of 3-6 specific strings describing what was found>],
  "paragraphs": [
    {
      "index": <integer matching paragraph index>,
      "score": <integer 0-100, likelihood this paragraph is AI-generated>,
      "flag": "<one of: 'human', 'mixed', 'ai'>",
      "reasoning": "<1 short sentence explaining the score>"
    }
  ]
}

Where overall_score is 0 = entirely human-written, 100 = entirely AI-generated.`;

    const message = await client.messages.create({
      model: 'claude-opus-4-6',
      max_tokens: 4096,
      messages: [
        {
          role: 'user',
          content: prompt,
        },
      ],
    });

    const responseText = message.content[0].type === 'text' ? message.content[0].text : '';

    // Parse JSON response
    let analysis;
    try {
      // Extract JSON from the response (handle cases where there's surrounding text)
      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      if (!jsonMatch) throw new Error('No JSON found');
      analysis = JSON.parse(jsonMatch[0]);
    } catch {
      return NextResponse.json(
        { error: 'Failed to parse analysis response. Please try again.' },
        { status: 500 }
      );
    }

    // Attach original paragraph texts
    analysis.paragraphs = analysis.paragraphs.map((p: { index: number; score: number; flag: string; reasoning: string }) => ({
      ...p,
      text: paragraphs[p.index] || '',
    }));

    return NextResponse.json({
      ...analysis,
      truncated,
      total_paragraphs: paragraphs.length,
      file_name: file.name,
    });
  } catch (error: unknown) {
    console.error('Analysis error:', error);
    const message = error instanceof Error ? error.message : 'Unknown error';
    return NextResponse.json(
      { error: `Analysis failed: ${message}` },
      { status: 500 }
    );
  }
}
