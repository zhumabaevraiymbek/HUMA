import { NextRequest, NextResponse } from 'next/server';
import Anthropic from '@anthropic-ai/sdk';

export const maxDuration = 60;
export const runtime = 'nodejs';

const apiKey = process.env.ANTHROPIC_API_KEY;
if (!apiKey) console.error('ANTHROPIC_API_KEY is not set');

const client = new Anthropic({ apiKey });

async function extractTextFromDocx(buffer: Buffer): Promise<string> {
  const mammoth = await import('mammoth');
  const result = await mammoth.extractRawText({ buffer });
  return result.value;
}

function splitIntoParagraphs(text: string): string[] {
  return text
    .split(/\n{2,}/)
    .map(p => p.replace(/\n/g, ' ').trim())
    .filter(p => p.length > 60);
}

const LANG_INSTRUCTIONS = {
  en: 'Respond in English. All text fields (summary, reasoning, key_indicators, verdict, confidence) must be in English.',
  ru: 'Отвечай на русском языке. Все текстовые поля (summary, reasoning, key_indicators, verdict, confidence) должны быть на русском языке. Verdict должен быть одним из: "Скорее всего человек", "Вероятно человек", "Смешанный", "Вероятно ИИ", "Скорее всего ИИ". Confidence: "Низкая", "Средняя" или "Высокая".',
  kk: 'Қазақ тілінде жауап бер. Барлық мәтін өрістері (summary, reasoning, key_indicators, verdict, confidence) қазақ тілінде болуы керек. Verdict мыналардың бірі болуы керек: "Адам жазған", "Адам жазуы ықтимал", "Аралас", "AI жазуы ықтимал", "AI жазған". Confidence: "Төмен", "Орташа" немесе "Жоғары".',
};

export async function POST(req: NextRequest) {
  try {
    const formData = await req.formData();
    const file = formData.get('file') as File | null;
    const lang = (formData.get('lang') as string) || 'en';

    if (!file) return NextResponse.json({ error: 'No file provided' }, { status: 400 });

    const fileName = file.name.toLowerCase();
    if (!fileName.endsWith('.docx') && !fileName.endsWith('.doc')) {
      return NextResponse.json({ error: 'Only .docx and .doc files are supported' }, { status: 400 });
    }

    const arrayBuffer = await file.arrayBuffer();
    const buffer = Buffer.from(arrayBuffer);

    let rawText: string;
    try {
      rawText = await extractTextFromDocx(buffer);
    } catch {
      return NextResponse.json({ error: 'Failed to parse document.' }, { status: 422 });
    }

    if (!rawText || rawText.trim().length < 100) {
      return NextResponse.json({ error: 'Document is empty or too short (minimum 100 characters).' }, { status: 422 });
    }

    const maxChars = 40000;
    const truncated = rawText.length > maxChars;
    const textToAnalyze = truncated ? rawText.slice(0, maxChars) : rawText;
    const paragraphs = splitIntoParagraphs(textToAnalyze);

    if (paragraphs.length === 0) {
      return NextResponse.json({ error: 'Could not extract paragraphs from document.' }, { status: 422 });
    }

    const langInstruction = LANG_INSTRUCTIONS[lang as keyof typeof LANG_INSTRUCTIONS] || LANG_INSTRUCTIONS.en;

    const prompt = `You are an expert forensic linguist specializing in detecting AI-generated text. Analyze the document below and determine what percentage was likely generated by an AI language model.

LANGUAGE INSTRUCTION: ${langInstruction}

DOCUMENT TEXT:
---
${textToAnalyze}
---

PARAGRAPHS TO ANALYZE:
${paragraphs.map((p, i) => `[${i}] ${p}`).join('\n\n')}

Analyze for these AI signals:
- Unnaturally uniform sentence length and structure
- Overuse of transitional phrases
- Hedge language patterns
- Lack of personal anecdotes or genuine emotion
- Suspiciously perfect grammar throughout
- Generic, non-specific claims
- Repetitive semantic patterns
- Overly balanced structures
- Absence of colloquialisms or natural quirks

Respond ONLY with valid JSON (no markdown, no text outside JSON):

{
  "overall_score": <integer 0-100>,
  "verdict": "<translated verdict string>",
  "confidence": "<translated confidence: Low/Medium/High>",
  "word_count": <integer>,
  "summary": "<2-3 sentence summary in the target language>",
  "key_indicators": [<3-6 strings in target language>],
  "paragraphs": [
    {
      "index": <integer>,
      "score": <integer 0-100>,
      "flag": "<human|mixed|ai>",
      "reasoning": "<1 sentence in target language>"
    }
  ]
}`;

    const message = await client.messages.create({
      model: 'claude-haiku-4-5-20251001',
      max_tokens: 4096,
      messages: [{ role: 'user', content: prompt }],
    });

    const responseText = message.content[0].type === 'text' ? message.content[0].text : '';

    let analysis;
    try {
      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      if (!jsonMatch) throw new Error('No JSON found');
      analysis = JSON.parse(jsonMatch[0]);
    } catch {
      return NextResponse.json({ error: 'Failed to parse analysis response.' }, { status: 500 });
    }

    analysis.paragraphs = analysis.paragraphs.map((p: { index: number; score: number; flag: string; reasoning: string }) => ({
      ...p,
      text: paragraphs[p.index] || '',
    }));

    return NextResponse.json({
      ...analysis,
      truncated,
      total_paragraphs: paragraphs.length,
      file_name: file.name,
    });
  } catch (error: unknown) {
    console.error('Analysis error:', error);
    const message = error instanceof Error ? error.message : 'Unknown error';
    return NextResponse.json({ error: `Analysis failed: ${message}` }, { status: 500 });
  }
}
